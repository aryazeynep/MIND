#!/usr/bin/env python3
"""
Lazy-Loading Universal Dataset

Memory-efficient dataset that loads PyG data on-demand from disk.
Ideal for large-scale datasets (1M+ samples) that don't fit in RAM.

Usage:
    from lazy_universal_dataset import LazyUniversalDataset
    
    dataset = LazyUniversalDataset(
        chunk_pt_files=[
            'processed/chunk_0/processed/data.pt',
            'processed/chunk_1/processed/data.pt',
            ...
        ],
        transform=my_transform
    )
    
    # Works with PyTorch DataLoader
    loader = DataLoader(dataset, batch_size=8, shuffle=True)
"""

import torch
from torch.utils.data import Dataset
from torch_geometric.data import Data
from torch_geometric.data.separate import separate
from collections import OrderedDict
from pathlib import Path
from typing import List, Optional, Callable
from tqdm import tqdm


class LazyUniversalDataset(Dataset):
    """
    Lazy-loading dataset for PyG data stored in multiple .pt files.
    
    Key Features:
    - Memory-efficient: Only loads chunks on-demand
    - LRU caching: Keeps most recently used chunks in memory
    - Compatible with PyTorch DataLoader and random_split
    - Supports transforms
    
    RAM Usage:
    - Metadata: ~10KB per chunk
    - Cache: max_cache_chunks × chunk_size × 20KB (default: ~1.5GB)
    - Total: ~2GB for 1M proteins (vs ~50GB with InMemoryDataset)
    """
    
    def __init__(
        self,
        chunk_pt_files: List[str],
        transform: Optional[Callable] = None,
        max_cache_chunks: int = 3,
        verbose: bool = True
    ):
        """
        Initialize LazyUniversalDataset
        
        Args:
            chunk_pt_files: List of paths to processed .pt files
            transform: Optional transform to apply to each sample
            max_cache_chunks: Maximum number of chunks to keep in RAM (default: 3)
            verbose: Print loading information
        """
        # Filter out PyG metadata files (pre_transform.pt, pre_filter.pt)
        self.chunk_pt_files = [
            str(Path(f).resolve()) for f in chunk_pt_files 
            if not Path(f).name.startswith('pre_')
        ]
        self.transform = transform
        self.max_cache_chunks = max_cache_chunks
        self.verbose = verbose
        
        # LRU cache for loaded chunks
        self._cache = OrderedDict()
        
        # Build index map: which sample is in which file?
        self._build_index_map()
        
        if self.verbose:
            print(f"✅ LazyUniversalDataset initialized:")
            print(f"   Total chunks: {len(self.chunk_pt_files)}")
            print(f"   Total samples: {self.total_samples:,}")
            print(f"   Cache size: {self.max_cache_chunks} chunks")
            print(f"   Estimated cache RAM: ~{self._estimate_cache_ram():.0f}MB")
    
    def _build_index_map(self):
        """Build index map: (global_idx -> (file_idx, local_idx, pt_file))"""
        self.file_ranges = []
        self.samples_per_file = []
        cumsum = 0
        
        desc = "Building index map" if self.verbose else None
        for pt_file in tqdm(self.chunk_pt_files, desc=desc, disable=not self.verbose):
            if not Path(pt_file).exists():
                raise FileNotFoundError(f"Chunk file not found: {pt_file}")
            
            # Load only slices to count samples (minimal RAM)
            try:
                _, slices = torch.load(pt_file, map_location='cpu', weights_only=False)
            except Exception as e:
                raise RuntimeError(f"Failed to load {pt_file}: {e}")
            
            # Determine number of samples
            # Use 'pos' as reference (every sample must have positions)
            if 'pos' not in slices:
                raise ValueError(f"Invalid .pt file (no 'pos' in slices): {pt_file}")
            
            n_samples = len(slices['pos']) - 1
            self.samples_per_file.append(n_samples)
            self.file_ranges.append((cumsum, cumsum + n_samples, pt_file))
            cumsum += n_samples
            
            # Free memory immediately
            del slices
        
        self.total_samples = cumsum
        
        if self.total_samples == 0:
            raise ValueError("No samples found in any chunk files!")
    
    def _estimate_cache_ram(self):
        """Estimate RAM usage for cache (in MB)"""
        # Rough estimate: 20KB per protein sample
        avg_samples_per_chunk = sum(self.samples_per_file) / len(self.samples_per_file)
        ram_per_chunk_mb = avg_samples_per_chunk * 20 / 1024  # KB to MB
        total_cache_ram = ram_per_chunk_mb * self.max_cache_chunks
        return total_cache_ram
    
    def __len__(self):
        """Return total number of samples"""
        return self.total_samples
    
    def __getitem__(self, idx: int) -> Data:
        """
        Get a single sample by index.
        
        This method:
        1. Finds which chunk file contains the sample
        2. Loads the chunk (or retrieves from cache)
        3. Extracts the specific sample using PyG's separate() function
        4. Applies transform if specified
        
        Args:
            idx: Global sample index
            
        Returns:
            PyG Data object
        """
        if idx < 0 or idx >= self.total_samples:
            raise IndexError(f"Index {idx} out of range [0, {self.total_samples})")
        
        # Find which file contains this sample
        for start_idx, end_idx, pt_file in self.file_ranges:
            if start_idx <= idx < end_idx:
                local_idx = idx - start_idx
                
                # Load chunk if not in cache
                if pt_file not in self._cache:
                    self._load_chunk(pt_file)
                else:
                    # Move to end (most recently used)
                    self._cache.move_to_end(pt_file)
                
                # Extract sample
                data, slices = self._cache[pt_file]
                sample = self._extract_sample(data, slices, local_idx)
                
                # Apply transform
                if self.transform is not None:
                    sample = self.transform(sample)
                
                return sample
        
        # Should never reach here
        raise IndexError(f"Could not locate index {idx}")
    
    def _load_chunk(self, pt_file: str):
        """Load a chunk file into cache"""
        # Evict oldest chunk if cache is full
        if len(self._cache) >= self.max_cache_chunks:
            oldest_file = next(iter(self._cache))
            del self._cache[oldest_file]
        
        # Load new chunk
        data, slices = torch.load(pt_file, map_location='cpu', weights_only=False)
        self._cache[pt_file] = (data, slices)
    
    def _extract_sample(self, data: Data, slices: dict, idx: int) -> Data:
        """
        Extract a single sample from collated data.
        
        This uses PyG's separate() function which handles:
        - Tensor slicing based on slices dictionary
        - Edge index offsetting
        - Heterogeneous data structures
        
        Args:
            data: Collated PyG Data object
            slices: Slices dictionary
            idx: Local index within this chunk
            
        Returns:
            Individual PyG Data object
        """
        # Use PyG's built-in separate function
        sample = separate(
            cls=data.__class__,
            batch=data,
            idx=idx,
            slice_dict=slices,
            inc_dict=None,
            decrement=False
        )
        
        return sample
    
    def get_chunk_stats(self):
        """Get statistics about chunks"""
        return {
            'num_chunks': len(self.chunk_pt_files),
            'total_samples': self.total_samples,
            'samples_per_chunk': self.samples_per_file,
            'avg_samples_per_chunk': sum(self.samples_per_file) / len(self.samples_per_file),
            'cache_size': self.max_cache_chunks,
            'cached_chunks': list(self._cache.keys()),
        }
    
    def clear_cache(self):
        """Clear the chunk cache to free memory"""
        self._cache.clear()
    
    def get_chunk_indices(self, chunk_idx: int) -> List[int]:
        """
        Get all global sample indices for a specific chunk.
        
        Used by ChunkAwareSampler to enable sequential chunk reading.
        
        Args:
            chunk_idx: Index of the chunk (0 to num_chunks-1)
            
        Returns:
            List of global sample indices in this chunk
        """
        if chunk_idx < 0 or chunk_idx >= len(self.chunk_pt_files):
            raise IndexError(f"Chunk index {chunk_idx} out of range [0, {len(self.chunk_pt_files)})")
        
        start_idx, end_idx, _ = self.file_ranges[chunk_idx]
        return list(range(start_idx, end_idx))
