seed: 42
dataset: LBA
dataset_download_dir: ./data
out_path: ./outputs/universal_pretrain_lba
wandb_project_name: esa-universal-pretraining
wandb_run_name: universal-LBA-pretraining
max_epochs: 100
gradient_clip_val: 0.5
monitor_loss_name: train_total_loss

num_workers: 4
molecule_max_atoms: 500  # LBA has larger molecules

# Universal representation specific settings
use_universal_cache: true
universal_cache_path: "./data/processed/raw/universal_lba_all.pkl"

apply_attention_on: node
graph_dim: 512
edge_dim: 64
hidden_dims: [512, 512, 512, 512,512,512,512,512]
num_heads: [8, 8, 8, 8,8,8,8,8]
layer_types: ["M", "S", "S", "M","S","M","S","P"]
xformers_or_torch_attn: xformers
norm_type: LN
use_3d_coordinates: true

use_hierarchical_features: true
block_embedding_dim: 32
position_embedding_dim: 16
entity_embedding_dim: 8
max_block_types: 150  # GET vocabulary size (4 special + 20 AA + 8 bases + 118 atoms)
max_position_types: 15  # GET position codes (15 total)
max_entity_types: 6  # protein, ligand, DNA, RNA, small molecule, other

# MLP config
use_mlps: true
mlp_hidden_size: 512
num_mlp_layers: 3

# Hardcoded values that are now configurable
max_atomic_num: 118  # GET atomic elements vocabulary (118 chemical elements from h to og)
rwse_dim: 24  # RWSE positional encoding dimension
lap_dim: 4  # LapPE positional encoding dimension
mlp_inter_dim: 128  # MLP intermediate dimension
pma_num_outputs: 32  # PMA (Pooling by Multihead Attention) outputs
max_node_items: 200  # Maximum nodes for attention (increased for protein-ligand complexes)
max_edge_items: 2000  # Maximum edges for attention (increased for protein-ligand complexes)
output_dim: 1  # Output dimension for final predictions
mlp_type: standard
mlp_dropout: 0.0
use_mlp_ln: false
pre_or_post: pre
use_bfloat16: true

# Dropout settings
sab_dropout: 0.0
mab_dropout: 0.0
pma_dropout: 0.0
attn_residual_dropout: 0.0
pma_residual_dropout: 0.0

# ESA optimizations
set_max_items: 0
triu_attn_mask: false

# Positional encoding
use_posenc: false  # Set to true to enable RWSE+LapPE positional encoders

gaussian_kernels: 128
cutoff_distance: 5.0
max_neighbors: 16
max_distance: 10.0  # Appropriate for protein-ligand complexes
distance_bins: 16  # Fewer bins for easier learning

# Pretraining tasks matching working model
pretraining_tasks: ["short_range_distance", "mlm", "long_range_distance"]
task_weights:
  short_range_distance: 1.0  # Local chemical bond learning
  mlm: 0.5                   # Masked language modeling helps representation learning
  long_range_distance: 0.8   # Global 3D structure learning (reduced for protein-ligand complexity)

mlm_mask_ratio: 0.15
temperature: 0.1

lr: 0.0005  # Match working model exactly
early_stopping_patience: 30
optimiser_weight_decay: 0.0000000001  # Match working model exactly

# Optimized batch size for LBA (smaller due to larger molecules)
batch_size: 16

# Dataset size limit (use all cached LBA samples)
max_samples: 3507

# Memory efficient settings
use_memory_efficient_attention: true
attention_dropout: 0.1

# Universal representation features
# Note: Hierarchical blocks are ALWAYS available in universal representations
# They are pre-processed in the adapter, no need for GET processing during training
preserve_universal_blocks: true

# Fast training optimizations (since data loading is instant)
fast_dev_run: false
accelerator: auto
devices: auto
precision: bfloat16

# Disable sanity check for faster startup
num_sanity_val_steps: 0
